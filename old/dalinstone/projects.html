<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dalin Stone | Projects</title>
</head>
<body>
    <div>
        <header>
            <a href="index.html" id="logo_link">
                <img src="images/D$.png" alt="Dalin Stone Logo">
            </a>
            <nav>
                <a href="index.html">Home</a>
                <a href="projects.html">Projects</a>
                <a href="skills.html">Skills</a>
                <a href="resume.html">Resume</a>
            </nav>
        </header>
        <main>
            <div>
                <img src="images/leaf-background.jpg" alt="Neon leaf background">
            </div>
            <div></div>
            <div>
                <h1>Projects</h1>
            </div>
            <section>
                <h2>My Projects</h2>
            </section>
            <div>
                <img src="images/shaka.png" alt="a hand holding the shaka pose">
            </div>
            <div>
                <img src="images/idea.png" alt="a light bulb">
            </div>
            <section>
                <h3>Security Analyst - Abnormal Security</h3>
                    <ul>
                    
                        <li>
                            After asking my manager what I could take off her plate, the response was, "We need a way to better track working hours." I quickly developed a Google Mobile App in Google Cloud that could be downloaded onto any phone and accept input of a users working hours that would be tracked in a Google Sheet. However, the cost of the app for the company was too much, sending me back to the drawing board. I then wrote a script in JavaScript that would automatically send emails to everyone on the team, where they could record their working hours in a Google Form. This solution didn't see a lot of usage by the team. Finally, I developed a Slack integration that would ping the team 10 minutes before their shift reminding them to fill out the Google Form with their working hours that day.
                        </li>
                        <li>
                            Talking with a co-worker about how we could move into a data focused role we agreed that we needed to put in the hard work. We met every Tuesday, Wednesday, and Thursday for 2 months to study SQL. These study sessions prompted us to think about how we could do something more meaningful. We looked into improving the way our team was tracking and using data. As we were migrating data from Grafana to Databricks we created dashboards that would allow the team to track important KPI and SLA metrics on the team. This collaboration allowed us to involve the entire team during a 'Hackathon' week that we used to improve these dashboards.
                        </li>
                    
                    </ul>
            </section>
            <section>
                <h3>Quality Assurance Analyst - Abnormal Security</h3>
                    <ul>
                        <li>
                            I was tasked with developing a quality assurance program that could be used to increase the performance of the Security Investigations team in their remediation efforts. This program included developing a score card and a PIP template that would be referenced to help team members. In order to better understand how we could support the team in this effort to increase team wide KPI and SLA metrics in order to achieve OKRs, I met with the Director of Security Investigations, an assigned Data Scientist, Software Engineer, and Product Manager. A application was developed using K-Means modeling that would act as a testing environment for the team, where Analysts could practice remediating investigations without impacting the customer. The overall development and implementation of the program increased analyst performance by 4%-9%.
                        </li>
                    </ul>
            </section>
            <section>
                <h3>Senior Analyst - Goldman Sachs</h3>
                <ul>
                    <li>
                        After joining the team and coming up to speed on how to perform Anti-money laundering investigations, I found a foothold in being able to use data to assist the team. Instead of waiting 1-2 weeks for investigation data, I was able to pull data in only 2 days and filter that data into the scope of the investigation. Using Databricks, Aqua Data Studio, and Snowflake I was able to pull data for an investigation in the Consumer Business line that was eventually presented to Managing Director and Partner stakeholders at the firm.
                    </li>
                    <li>
                        I assisted in investigations ranging from Political Contributions to Cryptocurrency, being involved in the planning, scoping, and analysis of the proposed investigations.
                    </li>
                </ul>
            </section>
            <section>
                <h3>Data Engineer - Saela</h3>
                <ul>
                    <li>
                        Coming into the role, I spent a majority of my time handling Tableau tickets that involved the administration and management of Tableau Server, Tableau Desktop, and Tableau Prep for 100+ users. Ticket management was performed using JIRA.
                    </li>
                    <li>
                        After learning about the business processes, I was tasked with performing QA on data model development. I implemented a process using Python that shortened QA time by 90% saving hours of time.
                    </li>
                    <li>
                        I created tool processes in Python that would allow the team to read and write data 95% faster than the previously employed method. Other tools included the ability to interact with the Tableau API using Python and a method of using Dask to pull data efficiently from MySQL databases.
                    </li>
                    <li>
                        I created several data models using Sales, Customer, and Employee data in Python using Prefect, Pandas, NumPy, the Tableau API, and distributed computing packages. After creating the data models, I migrated the data into a Tableau workbook that could be consumed by executive level and manager stakeholders.
                    </li>
                    <li>
                        I was tasked with helping in the setup and maintenance of Docker to deploy the Prefect data pipeline to an AWS EC2 server running AWS-Linux. We automated the deployment by initializing 'supervisor' on the linux server.
                    </li>
                </ul>
            </section>
            <section>
                <h3>SOC Intern - The Church of Jesus Christ of Latter-day Saints</h3>
                <ul>
                    <li>

                    </li>
                </ul>
            </section>
        </main>
        <footer>
            <p>Dalin Stone</p>
            <p><a href="site-plan.html">Site Plan</a></p>
            <p><a href="resume.html">Resume</a></p>
            <p>Icons created by <a href="https://www.flaticon.com/free-icons/idea">Smashicons</a>,<a href="https://www.flaticon.com/free-icons/snakes">Freepik</a>, and <a href="https://www.flaticon.com/free-icons/analysis">monkik</a> - Flaticon.</p>            <div>
                <a href="index.html" id="logo_link">
                    <img src="images/D$.png" alt="Dalin Stone Logo">
                </a>
            </div>
        </footer>
    </div>
</body>
</html>